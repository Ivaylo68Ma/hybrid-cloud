= Hybrid Cloud

image:https://github.com/redhat-developer-demos/hybrid-cloud/workflows/backend/badge.svg[]
image:https://github.com/redhat-developer-demos/hybrid-cloud/workflows/frontend/badge.svg[]

*Tested with skupper 0.2.0*

=== Install Skupper

[source, shell-session]
----
# Linux
curl -fL https://github.com/skupperproject/skupper-cli/releases/download/0.2.0/skupper-cli-0.2.0-linux-amd64.tgz | tar -xzf -

#Â MacOs
curl -fL https://github.com/skupperproject/skupper-cli/releases/download/0.2.0/skupper-cli-0.2.0-linux-amd64.tgz | tar -xzf -
----

=== Deploy Services

The deployment of services to all ACM managed clusters will be done using GitOps. 

To deploy backend to the designated clusters, follow the https://github.com/redhat-developer-demos/demos-acm-manifests/tree/master/hybrid-demo#deploying-backend[instructions].


Deploy the frontend to your *frontend* cluster using the https://github.com/redhat-developer-demos/demos-acm-manifests/tree/master/hybrid-demo#deploying-frontend[instructions].

Run `oc expose service hybrid-cloud-frontend` after deploying frontend resource, and it is not required to modify the Ingress configuration. 

In your *frontend* cluster, init `skupper` and create the `connection-token`:

[source, bash]
----
skupper init --console-auth unsecured # <1>

Skupper is now installed in namespace 'default'.  Use 'skupper status' to get more information.
----

<1> This makes anyone be able to access the Skupper UI to visualize the clouds. Fine for demos, not to be used in production.

[source, bash]
----
watch skupper status
----

See the status of the skupper pods. It takes a bit of time (usually around 2 minutes) until the pods are running:

[source, bash]
----
oc get pods 
----

[source, bash]
----
NAME                                        READY   STATUS    RESTARTS   AGE
hybrid-cloud-backend-5948955b7-m5dzk        1/1     Running   0          16m
hybrid-cloud-backend-proxy-89f44c75-vx9kn   1/1     Running   0          51s
hybrid-cloud-frontend-7c85f9dfff-fmbd6      1/1     Running   0          10m
skupper-proxy-controller-77f6c568cc-f8xxd   1/1     Running   0          102s
skupper-router-7976948d9f-hn6ms             1/1     Running   0          104s
----

Finally create a token:

----
skupper connection-token token.yaml

Connection token written to token.yaml
----

In *all the other clusters*, use the connection token created in the previous step:

[source, shell-session]
----
skupper init
skupper connect --connection-name <connection-name> token.yaml #<1>
----

<1> Connect the cluster with a name and token

Everything is connected and ready to be used. To verify the connections on each connected run the following command:

[source, shell-session]
----
skupper list-connectors
----

=== Skupper UI

If you run:

[source, shell-session]
----
oc get services 

NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP                                                                  PORT(S)               AGE
hybrid-cloud-backend    ClusterIP      172.30.32.251    <none>                                                                       8080/TCP              40m
hybrid-cloud-frontend   LoadBalancer   172.30.25.65     add076df5798711eaad1a0241cddbab7-1371911574.eu-central-1.elb.amazonaws.com   8080:32647/TCP        39m
kubernetes              ClusterIP      172.30.0.1       <none>                                                                       443/TCP               71m
openshift               ExternalName   <none>           kubernetes.default.svc.cluster.local                                         <none>                70m
skupper-controller      ClusterIP      172.30.247.104   <none>                                                                       8080/TCP              34m
skupper-internal        ClusterIP      172.30.109.84    <none>                                                                       55671/TCP,45671/TCP   34m
skupper-messaging       ClusterIP      172.30.64.245    <none>                                                                       5671/TCP              34m
----

You'll notice that there is a `skupper-controller` service which is the entry point for the Skupper UI.
Expose this service so it is reachable from outside the cluster and you'll be able to access the Skupper UI.
